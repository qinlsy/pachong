from concurrent.futures import ThreadPoolExecutor

from goose3 import Goose
import json
import re
from htmldate import find_date
import requests
import urllib.parse
from lxml import etree
from gne import GeneralNewsExtractor
from fake_useragent import UserAgent
import urllib3
from newspaper import Article

headers = {'user-agent': UserAgent().random }


#
def get_html(url):
    try:
        r_text = requests.get(url=url, headers=headers, timeout=(5, 10), verify=False)
        r_text.encoding = r_text.apparent_encoding
        r_text = r_text.text
        return r_text
    except:
        return None


def save(info):
    with open(f'{id}.txt', 'a+', encoding='utf-8') as f:
        f.write(json.dumps(info, ensure_ascii=False) + '\n')
        print(info)


def run(url):
    try:
        g = Goose({'browser_user_agent': headers})
        url = "".join(url).strip()
        # print(f"开始解析{url}")
        html = get_html(url)
        # 获取文章内容
        article = g.extract(url, html)
        # 标题
        title = article.title
        txt = article.cleaned_text
        if title != "" and txt != "":
            dit = {}
            dit["title"] = article.title
            dit["txt"] = article.cleaned_text
            dit["url"] = url
            save(dit)
        else:
            print("title or txt 有空跳过")
    except:
        pass


def goose(url, count=0):
    html = get_html(url)
    html_xpath = etree.HTML(html)
    next_links = html_xpath.xpath("//h3/a/@href")
    next_links = list(set(next_links))
    # 存在下一页则进行拼接
    if next_links:
        for next_link in next_links:
            next_link = 'https://lefaso.net/' + next_link
            run(next_link)

if __name__ == '__main__':
    with open("2794-2799.txt", mode="r", encoding="utf-8") as f:
        try:
            for data in f.readlines():
                # print(data)
                lst = data.split("----")
                id = lst[0]
                url = lst[1]
                rs = re.findall('rubrique(\d+)&debut', url)
                num = "".join(rs)
                ret = get_html(url)
                # print(ret)
                html = etree.HTML(ret)
                page = html.xpath('//span[@class="pages"]/span[@class="tbc"]/following-sibling::a/text()')
                page = "".join(page)
                print(f"id为{id},页数为:{page}")
                if page:
                    with ThreadPoolExecutor(max_workers=10) as pool:
                        for i in range(0, int(page) + 1, 20):
                            pool.submit(goose,f'https://lefaso.net/spip.php?rubrique{num}&debut_articles={i}#pagination_articles')
        except Exception as e:
            print(e)
